---
layout: post
title: "C√≥mo empec√© a hablar con la IA desde la aplicaci√≥n de escritorio gvSIG desktop"
date: 2025-09-08
canonical_url: "https://blog.gvsig.org/2025/09/08/como-empece-a-hablar-con-la-ia-desde-la-aplicacion-de-escritorio-gvsig-desktop/"
---

¬øAlguna vez has querido preguntarle a tu aplicaci√≥n qu√© datos tiene o qu√© puede hacer con ellos, usando tus propias palabras? Esa fue la pregunta que me hice a principios de este a√±o cuando empec√© a ver que los modelos de lenguaje (LLMs, Large Language Models o Modelos de Lenguaje Grande) pod√≠an hacer cosas incre√≠bles... pero siempre desde un navegador web, lejos de la aplicaci√≥n de escritorio.

Como muchos sab√©is, llevo a√±os trabajando en gvSIG Desktop, una aplicaci√≥n Java de escritorio con mucha historia y mucho c√≥digo. La idea de integrar capacidades de IA directamente en la aplicaci√≥n me pareci√≥ fascinante, pero ten√≠a un problema: todo lo que ve√≠a requer√≠a frameworks complejos de Python o soluciones en la nube que no encajaban con nuestra realidad, requerian versiones modernas de java o python con las que no podia contar.

As√≠ que me dije _"voy a intentar hacerlo a mi manera"_. Voy a contaros c√≥mo, en unos dias, consegu√≠ que _gvSIG desktop_ empezara a conversar con Gemini usando poco m√°s que las librer√≠as HTTP de Java y Jython.
## El primer paso, un chat simple pero funcional

Lo primero que necesitaba era lo m√°s b√°sico, una ventana de chat dentro de gvSIG desktop que pudiera enviar mensajes a la API de Gemini y mostrar las respuestas.

Para mantener las cosas simples, us√© el entorno de scripting y Jython para prototipar r√°pidamente la interfaz de usuario, y para la comunicaci√≥n con la API, me bast√≥ con las librer√≠as HTTP de Java.

El n√∫cleo de la comunicaci√≥n se reduc√≠a a esto: construir el payload JSON correcto y enviarlo. 

```python
from java.net import URL, HttpURLConnection
from java.io import BufferedReader, InputStreamReader, OutputStreamWriter
from javax.json import Json

class GeminiClient:
    def __init__(self):
        self.history = []
        self.api_key = "tu_api_key"
        self.api_url = "https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent"

    def send_message(self, user_prompt):
        # Construir la solicitud HTTP
        url = URL(self.api_url + "?key=" + self.api_key)
        conn = url.openConnection()
        conn.setRequestMethod("POST")
        conn.setRequestProperty("Content-Type", "application/json")
        conn.setDoOutput(True)
        
        # Construir el payload JSON
        payload = {
            "contents": [{
                "parts": [{"text": user_prompt}]
            }]
        }
        
        # Enviar la solicitud
        output_stream = conn.getOutputStream()
        writer = OutputStreamWriter(output_stream, "UTF-8")
        writer.write(Json.createObjectBuilder(payload).build().toString())
        writer.flush()
        
        # Leer la respuesta
        reader = BufferedReader(InputStreamReader(conn.getInputStream()))
        response = ""
        line = reader.readLine()
        while line is not None:
            response += line
            line = reader.readLine()
        
        return response
```

La magia estaba en construir el Json correcto que espera la API de Gemini. Pero una vez resuelto esto, en pocas horas ya ten√≠a una ventana de chat funcionando dentro de gvSIG.

![Primer prototipo funcionando](/assets/images/Como_empece_a_hablar_con_la_IA_desde_la_aplicacion_de_escritorio_gvSIG_desktop/articulo1_img1.png)

_El primer prototipo era sencillo, pero funcionaba. Pod√≠a escribir preguntas y obtener respuestas_.

## El desaf√≠o real, m√°s all√° del chat conversacional

Un chat conversacional est√° bien para hacer pruebas, pero no es muy √∫til en una aplicaci√≥n de escritorio. El verdadero reto era c√≥mo usar el LLM para algo pr√°ctico. En nuestro caso, se me ocurri√≥ que podr√≠a ayudar con la ficha de b√∫squeda de gvSIG, que aunque es potente, puede resultar compleja para los usuarios.

Pero aqu√≠ aparec√≠a otro problema ¬øc√≥mo le pido al LLM que trabaje con mis datos sin enviarle informaci√≥n sensible, sin enviarle los datos de las capas que tengo cargadas la la aplicaci√≥n? La soluci√≥n fue ense√±arle a entender la estructura de mis datos, no darle los datos en s√≠.

## El arte del prompting estructurado, ense√±√°ndole las reglas del juego

La verdadera magia no estaba en la conexi√≥n HTTP o en la _cacharreria_ Json que habia que hacer para hablar con el LLM, sino en el dise√±o del *system prompt* que conseguir√≠a que el LLM entendiera exactamente c√≥mo ten√≠a que comportarse, estaba programando con palabras. El prompt se convirti√≥ en el cerebro del sistema, las reglas del juego que transformaban un modelo de lenguaje gen√©rico en un especialista de gvSIG desktop.


El desaf√≠o era triple:

1.  **Contexto**: Darle al LLM informaci√≥n suficiente sobre la estructura de los datos
2.  **Comportamiento**: Definir exactamente c√≥mo deb√≠a responder para cada tipo de consulta
3.  **Formato**: Forzar una salida estructurada y parseable

El prompt base ten√≠a que establecer las reglas fundamentales.

La estructura del esqueleto principal del prompt que constru√≠ fue algo parecido a esto:

```python

BASE_PROMPT = u"""
Eres un asistente especializado en gvSIG Desktop. 
Responde EXCLUSIVAMENTE en castellano con objetos JSON v√°lidos.

TIPOS DE CONSULTA SOPORTADOS:
- "text": Para consultas generales
- "sql": Para consultas SQL sobre las tablas
- "plantuml": Para diagramas de entidad-relaci√≥n

REGLAS ESTRICTAS PARA SQL:
- ‚õî Nunca incluyas esquemas en las consultas
- ‚úÖ Usa solo SQL-92 est√°ndar  
- üåç Solo funciones espaciales de SQL/MM y OGC SFS
- üö´ Evita sintaxis no est√°ndar
- üìè Limita a 1000 resultados

ESTRUCTURA DE RESPUESTA OBLIGATORIA:
{
  "type": "tipo_de_respuesta",
  // campos adicionales seg√∫n el tipo
}

EJEMPLO SQL:
{
  "type": "sql",
  "sql": "SELECT * FROM tabla WHERE condici√≥n LIMIT 1000;",
  "title": "Descripci√≥n clara",
  "esValorEscalar": false
}

EJEMPLO PLANTUML:
{
  "type": "plantuml", 
  "diagram": "@startuml\\nentity Tabla\\n@enduml",
  "title": "Diagrama de entidades"
}
"""
```

Con estas reglas, el LLM ya no solo conversaba, sino que generaba instrucciones. La aplicaci√≥n actuaba como un int√©rprete: recib√≠a un JSON, miraba el campo "type" y sab√≠a exactamente qu√© hacer. Si recib√≠a {"type": "sql", ...}, sab√≠a que deb√≠a mostrar un bot√≥n que, al pulsarlo, ejecutar√≠a la consulta SQL. Si recib√≠a {"type": "plantuml", ...}, generar√≠a un bot√≥n para visualizar el diagrama.

## Los resultados, cuando todo empez√≥ a encajar

La primera vez que prob√© el sistema completo me resulto impactante. El LLM dej√≥ de ser un simple chatbot para convertirse en un motor de an√°lisis que respond√≠a a mis preguntas con acciones concretas. Una conversaci√≥n natural se traduc√≠a en operaciones reales dentro de gvSIG:

![Generaci√≥n de un diagrama Entidad-Relaci√≥n a partir de una petici√≥n en lenguaje natural](/assets/images/Como_empece_a_hablar_con_la_IA_desde_la_aplicacion_de_escritorio_gvSIG_desktop/articulo1_diagram1.png)

En esta captura se ve c√≥mo el usuario pregunta _"Puedes generarme un modelo entidad relaci√≥n con las tablas a las que tienes acceso?"_ y el sistema responde con un bot√≥n que al hacer clic muestra el diagrama solicitado.

![Visualizaci√≥n de los resultados de una consulta SQL en una tabla dentro de gvSIG](/assets/images/Como_empece_a_hablar_con_la_IA_desde_la_aplicacion_de_escritorio_gvSIG_desktop/articulo1_tabla1.png)

Aqu√≠ vemos c√≥mo la orden de _"Muestrame las placas que tienen un soporte de acero"_ el sistema genera una consulta SQL y responde con un bot√≥n que al hacer clic muestra el resultado en una tabla.

![Creaci√≥n de un gr√°fico de barras a trav√©s de una conversaci√≥n con el asistente de IA](/assets/images/Como_empece_a_hablar_con_la_IA_desde_la_aplicacion_de_escritorio_gvSIG_desktop/articulo1_grafica1.png)

Finalmente, una solicitud m√°s compleja como un gr√°fico de barras se traduce en la visualizaci√≥n de este en la aplicaci√≥n.

Cada respuesta del sistema correspond√≠a a un Json estructurado que el LLM generaba y que la aplicaci√≥n parseaba y ejecutaba, mostrando los resultados en tablas o gr√°ficos seg√∫n el caso.

## Detalles de implementaci√≥n para desarrolladores

### La comunicaci√≥n con la API de Gemini

Implement√© un cliente Gemini en Jython que maneja el historial de conversaci√≥n.

```python
def send_message(self, user_prompt, initial_prompt=None):
    # A√±adir mensajes al historial
    if not self.history and initial_prompt is not None:
        self.history.append({"role": "user", "parts": [{"text": initial_prompt}]})
    
    self.history.append({"role": "user", "parts": [{"text": user_prompt}]})
    
    # Construir y enviar payload
    payload = self._build_payload()
    response = self._send_request(payload)
    
    # Procesar respuesta
    generated_text = self._parse_response(response)
    self.history.append({"role": "model", "parts": [{"text": generated_text}]})
    
    return generated_text
```

### Procesamiento de respuestas Json

Una de las partes m√°s cr√≠ticas fue el parseo de las respuestas Json del LLM. Muchas veces el LLM mezclaba la respuesta estructurada junto con comentarios, preguntas o peticiones de aclaraciones o informaci√≥n, y habia que separar la respuesta extructurada, el Json, del resto de la conversaci√≥n.

```python
def extraer_json_de_markdown(texto_completo):
    marca_inicio = u'```json'
    marca_fin = u'```'

    pos_inicio = texto_completo.find(marca_inicio)
    if pos_inicio != -1:
        pos_contenido_inicio = pos_inicio + len(marca_inicio)
        pos_fin = texto_completo.find(marca_fin, pos_contenido_inicio)
        
        if pos_fin != -1:
            json_encontrado = texto_completo[pos_contenido_inicio:pos_fin].strip()
            return json_encontrado, texto_completo[:pos_inicio] + texto_completo[pos_fin + len(marca_fin):]
    
    return None, texto_completo
```

## Reflexiones... y una pregunta inc√≥moda

Este primer prototipo me demostr√≥ varias cosas importantes:

1.  **No necesitas frameworks complejos** para empezar a integrar IA en aplicaciones legacy.
2.  **El prompting estructurado es la clave**: Puedes "programar" el comportamiento de un LLM con instrucciones precisas.
3.  **Java/Jython y las librer√≠as est√°ndar son suficientes** para comunicarse con APIs modernas.

Ten√≠a una prueba de concepto que funcionaba. Pero mientras ve√≠a las respuestas de la IA aparecer en la pantalla, una pregunta empez√≥ a rondarme la cabeza, una que podia convertir este experimento en un simple juguete personal.

Cada una de esas respuestas _"m√°gicas"_ requer√≠a una llamada a una API de pago. ¬øC√≥mo iba a compartir esto con alguien sin que ni ellos ni yo nos arruin√°semos en el intento? ¬øPedirle a los usuarios que consiguieran su propia API Key solo para probar mi herramienta? No parecia realista.

De repente, el mayor desaf√≠o ya no era t√©cnico, sino de distribuci√≥n. Pero antes de poder resolver ese problema, necesitaba que la herramienta fuera tan √∫til, tan indispensable, que mereciera la pena luchar por ella. Ten√≠a que dejar de ser un chat para convertirse en un verdadero asistente que entendiera el *contexto* de lo que el usuario estaba haciendo en gvSIG desktop.

De eso, y de c√≥mo el contexto lo cambi√≥ todo, hablar√© en el siguiente art√≠culo.

Un saludo.


